{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import transformers\n",
    "from transformers import AutoTokenizer , DataCollatorWithPadding,AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import datasets as dt\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification , TrainingArguments , Trainer\n",
    "from evaluate import load\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b52a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('./NitelNicel.xlsx')\n",
    "df=df.drop(['Unnamed: 0'],axis=1)\n",
    "df=df.drop(187)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(x):\n",
    "    x=''.join((s for s in x if not s.isdigit()))\n",
    "    x=x.strip('-')\n",
    "    x=x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, if the function is applied to 3 columns separately, if the columns are to be combined and turned into a single column, it can also be applied to that single column.\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(data_cleaning)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(data_cleaning)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(data_cleaning)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c950e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"Nicel_Cevap_KaanAla\"] , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords from the data to reduce input size\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('turkish'))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_text=[]\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filtered_text.append(word)\n",
    "    text=\" \".join(filtered_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e885e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations to reduce input size \n",
    "punct_list = list(string.punctuation)\n",
    "def remove_punct(text):\n",
    "    words=word_tokenize(text)\n",
    "    text_filtered=[]\n",
    "    for word in words:\n",
    "        if word not in punct_list:\n",
    "            text_filtered.append(word)\n",
    "    text=\" \".join(text_filtered)\n",
    "    return text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying statistical base keyphrase function to the data because dataset is turkish\n",
    "import string\n",
    "import pke\n",
    "def keyphrase(text):\n",
    "    stoplist=list(string.punctuation)\n",
    "    extractor = pke.unsupervised.YAKE()\n",
    "    extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None,\n",
    "                        stoplist=stoplist)\n",
    "    extractor.candidate_selection(n=5)\n",
    "    window = 2\n",
    "    use_stems = False \n",
    "    extractor.candidate_weighting(window=window,\n",
    "                              use_stems=use_stems)\n",
    "    threshold = 0.8\n",
    "    keyphrases = extractor.get_n_best(n=3, threshold=threshold)\n",
    "    return keyphrases[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying lemmatization to the data to simplfy inputs\n",
    "import zeyrek\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "def lemmatizer(text):\n",
    "    words_lemmatize=[]\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word=analyzer.analyze(word)[0][0].lemma\n",
    "        words_lemmatize.append(word)\n",
    "    text=\" \".join(words_lemmatize)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfdf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Bert model accept labels as 0 base labels edited accordingly\n",
    "df = df.assign(labels = lambda x: (x['Nicel_Puan'] - 1 ))\n",
    "df=df.drop([\"Nicel_Puan\"],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0699e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing number of label values from 5 to 2 to apply 2 label approach.\n",
    "def label_reduction(label):\n",
    "    if label==0 or label==1:\n",
    "        label=0\n",
    "    else:\n",
    "        label=1\n",
    "    return label\n",
    "\n",
    "df= df[df['labels'] != 2]df[\"labels\"] = df[\"labels\"].apply(label_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#veriyi temizlerken oluşan düzensilikleri gidermek amacıyla oluşturulan fonksiyon.\n",
    "banned_words=[\"Unk\",\"mu\",\"mi\"]\n",
    "def remove_unk(text):\n",
    "        simplified_text=[]\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            if word in banned_words:\n",
    "                text=text.replace(word,\"\")\n",
    "        text=text.strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a80163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User can apply functions according to which approach he will choose.This part available for multi-column approaches.\n",
    "\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(remove_stopwords)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(remove_stopwords)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(remove_stopwords)\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(remove_punct)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(remove_punct)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(remove_punct)\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(keyphrase)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(keyphrase)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(keyphrase)\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(lemmatizer)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(lemmatizer)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(lemmatizer)\n",
    "\n",
    "df[\"Nitel_Soru\"] = df['Nitel_Soru'].apply(remove_unk)\n",
    "df[\"Nitel_Cevap\"] = df['Nitel_Cevap'].apply(remove_unk)\n",
    "df[\"Nicel_Soru\"] = df['Nicel_Soru'].apply(remove_unk)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f330d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column merging part.If multi column approaches going to be used first merging should be used\n",
    "#ıf single column approaches going to be used  then second merging should be used\n",
    "\n",
    "df[\"input\"]=\"[CLS]\"+\" \"+df[\"Nitel_Soru\"]+\" \"+\"[SEP]\"+\" \"+df[\"Nitel_Cevap\"]+\" \"+\"[SEP]\"+\" \"+df[\"Nicel_Soru\"]+\" \"+\"[SEP]\"\n",
    "df[\"input\"]=df[\"Nitel_Soru\"]+\" \"+df[\"Nitel_Cevap\"]+\" \"+df[\"Nicel_Soru\"]\n",
    "df=df.drop([\"Nitel_Soru\",\"Nitel_Cevap\",\"Nicel_Soru\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f56a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions are applied to the input columns.This part is available for single column approaches\n",
    "\n",
    "df[\"input\"] = df['input'].apply(keyphrase)\n",
    "df[\"input\"] = df['input'].apply(lemmatizer)\n",
    "df[\"input\"] = df['input'].apply(remove_unk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ef308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df.iloc[500:]\n",
    "df=df.iloc[:500]\n",
    "df.to_csv(\"classification_task_csv\", index=False , encoding='utf-8'  )\n",
    "df_test.to_csv(\"classification_task_test_csv\", index=False , encoding='utf-8'  )\n",
    "Tapaco_dataset=dt.load_dataset(\"csv\",data_files=\"classification_task_csv\")\n",
    "Tapaco_dataset=Tapaco_dataset[\"train\"]\n",
    "Tapaco_dataset.shuffle()\n",
    "Tapaco_dataset=Tapaco_dataset.train_test_split(test_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabaa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint=\"dbmdz/bert-base-turkish-uncased\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "max_input_length=512 #512 ve 1024 arasında değişiyor maksimum izin verilen uzunluk , garanti olsun diye 512 yaptım.\n",
    "\n",
    "def tokenizer_function(example) :\n",
    " \n",
    "    model_inputs = tokenizer(example[\"input\"], max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset=Tapaco_dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "model_name = \"BertTürk_Classification\"\n",
    "model_dir = f\"./{model_name}\"\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy= load(\"accuracy\")\n",
    "\n",
    "def metrics_display(eval_pred):\n",
    "    predictions , labels = eval_pred\n",
    "    predictions=np.argmax(predictions,axis=1)\n",
    "    return accuracy.compute(predictions=predictions,references=labels)\n",
    "\n",
    "def model_init():\n",
    "    return BertForSequenceClassification.from_pretrained(model_checkpoint,num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb913e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metrics_display,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aad1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
